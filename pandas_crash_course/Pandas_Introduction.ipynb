{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series is nothing but an array with named index. Using this named index we can grab the data from the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['a', 'b', 'c']\n",
    "mylist = [10, 20, 30]\n",
    "arr = np.array([10, 20, 30])\n",
    "dict_ = {'a': 10, 'b':20, 'c':30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data= mylist, index=None) # converting a python list to pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data= mylist, index=labels) # converting a python list to pandas series with index labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=arr) # converting a numpy array to pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(data=arr, index=labels)  # converting a numpy array to pandas series with index lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict_)  # converting a dictionary to pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ1 = pd.Series(data=[20,30,40,50], index=['IN','USA','NZ', 'BRA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ2 = pd.Series(data=[200,400,300,345], index=['IN','USA','NZ','CHA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ2['IN']  # Getting india sells based on the index lable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ2[0] # Getting india sells based on the integer index of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ2[4] # invalid key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesQ1 + salesQ2  # we will get the NaN for the lables for which information is not available in both series\n",
    "                   # note: we ca change this behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple pandas series that shares the same index, think of it as spreadsheet but much more powerfu;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['w','x','y','z']  # features or lables or columns\n",
    "index = ['a','b','c','d','e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data =randint(-100, 100, size = (5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, index=index, columns=column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['w'], type(df['w'])  # selecting the whole column (this is  series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['w','z']]  # selecting multiple columns (this will return dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new'] = df['w'] + df['y']  # adding a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels='new', axis=1)  # not an inplace dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels='new', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a']  # selecting a raw based on index label name, ruturn a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['a','e']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0] # selecting a row based on integer index, ruturn a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1]  #last row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[1:4] # selecting row 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('c', axis=0)  #dropping a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['a','w']  # selecting a value from the dataframe (from row 'a', column 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['a','c'],'w'] # selecting row values of a same column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['a','c'],['w','z']]  # selecting multiple values of muliple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df > 0  # selecitng df based on some condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df>0]  # filtering the dataframe based on some condition, we will get NaN where this condition does not met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x']>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['x']>0]  ## selecting all the raws where 'x' column vlaues is greater than zero, return a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['x']>0]['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['x']>0]['w']['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['x']>0) & (df['w']>2)]  ## filtering based on multiple conditions but using and (&) condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['x']>0) | (df['w']>2)]  #3 filtering based on multiple condition but using or (|) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()  # this will create a new column with the original index in a column named 'index' and it will replace the old\n",
    "                    # index with integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index=['AA','BB','CC','DD','EE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_index'] = new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('new_index')  # it will set the provided column name as new index, if not found will throw an exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('new_index') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index  # getting the index names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns    # getting column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()  ## return the count ,mean,std, min and percentile of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deal with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on our business or product goals we need to remove the missing data or fill the mssing data with mean, median, or with some other strategy or we can keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':[1,2,np.nan],\n",
    "                  'b':[2,np.nan,7],\n",
    "                  'c':[6,np.nan,np.nan]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()  ## checking if the nan value is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()  # alias of isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any() #3 return ture if a column contains any missing vlaues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().all()  #3 will return true if all the values in a column is missing else false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()  # retunr how many missing values in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()  # drop all the raws that ciontains the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)  # drop all the columns that contrains the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, how='all')  # drop all the column which contains all the values as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, how='any', thresh=2)  # drop all the column which contains atleaset 2 NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value='fillvalue') #3 filling the NaN value as 'fillvalue' string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=0) # filling the NaN value as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['a'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  df['a'].fillna(value=df['a'].mean()) # filling the value with column mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=df.mean())  # filling the vlaue with df mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupby "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "often we want to how vlaues are distributed or aggregated across groups, so thats why we use the groupby method.\n",
    "\n",
    "<b>Note: often this is also known as ``split-apply-combine``` technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the operation with groupby  call must be an aggregation method. this means that it can take multiple values and combined them to return a single vlaue. example: sum, mean, count, max etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Universities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()  #3 shwoing top 5 rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes  #3 return the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Year').sum()  # return completions acroos each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Year').mean() # return avg completions accross each year,  it wil return a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.groupby('Year').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Some other aggregate functions in pandas\n",
    "\n",
    "<table><td><tt\n",
    "><span\n",
    ">count</span></tt></td><td>Number of non-null observations</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">sum</span></tt></td><td>Sum of values</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">mean</span></tt></td><td>Mean of values</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">mad</span></tt></td><td>Mean absolute deviation</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">median</span></tt></td><td>Arithmetic median of values</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">min</span></tt></td><td>Minimum</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">max</span></tt></td><td>Maximum</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">mode</span></tt></td><td>Mode</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">abs</span></tt></td><td>Absolute Value</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">prod</span></tt></td><td>Product of values</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">std</span></tt></td><td>Unbiased standard deviation</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">var</span></tt></td><td>Unbiased variance</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">sem</span></tt></td><td>Unbiased standard error of the mean</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">skew</span></tt></td><td>Unbiased skewness (3rd moment)</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">kurt</span></tt></td><td>Unbiased kurtosis (4th moment)</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">quantile</span></tt></td><td>Sample quantile (value at %)</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">cumsum</span></tt></td><td>Cumulative sum</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">cumprod</span></tt></td><td>Cumulative product</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">cummax</span></tt></td><td>Cumulative maximum</td></tr><tr\n",
    "><td><tt\n",
    "><span\n",
    ">cummin</span></tt></td><td>Cumulative minimum</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['Year','Sector']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Year').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Year').describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some useful operations in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_one = pd.DataFrame({'k1':['A','A','B','B','C','C'],\n",
    "                      'col1':[100,200,300,300,400,500],\n",
    "                      'col2':['NY','CA','WA','WA','AK','NV']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['col1'].unique()  ## unique value in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['col1'].nunique() ## total number of unique value in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['col1'].value_counts()  # number of counts per unique value\n",
    "                                # df_one.groupby('col1')['col1'].count() alsome asame as this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.drop_duplicates()  # it will drop the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['New Col'] = df_one['col1'] * 10  # creating a new col by multiplying 'col1' by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_first_letter(state):\n",
    "    # Given a state, return the first letter\n",
    "    return state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_first_letter('NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice we only pass the function, we don't call it with ()\n",
    "df_one['col2'].apply(grab_first_letter)  # using apply function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['first letter'] = df_one['col2'].apply(grab_first_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_letter(state):\n",
    "    \n",
    "    if state[0] == \"W\":\n",
    "        return \"Washington\"\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['State Check'] = df_one['col2'].apply(complex_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WATCH OUT FOR DATA TYPE ERRORS!\n",
    "# You can't index numbers!\n",
    "df_one['col1'].apply(complex_letter)  # this will throw an error because we are trying to get the first letter of an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['k1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['k1'].map({'A':1,'B':2,'C':3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['col1'].max() , df_one['col1'].idxmax()  ## getting the max value and it's index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one['col1'].min() , df_one['col1'].idxmin()  ## getting the min value and it's index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.sort_values('col2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### concatenating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame({'A':[100,200,300,400,500],\n",
    "                        'B':[12,13,14,15,16]})\n",
    "predictions = pd.DataFrame({'pred':[0,1,1,0,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pay careful attention to the axis parameter!\n",
    "pd.concat([features,predictions])  # concenating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([features,predictions],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one.columns = ['C1','C2','C3','C4','C5','C6']\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting dummy values\n",
    "pd.get_dummies(df_one['C1'])  ## for each category it will create a column and it will fill ones where it was present else zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out the references here! \n",
    "\n",
    "**This is the best online resource for how to read/write to a variety of data sources!**\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"colwidths-given docutils\">\n",
    "<colgroup>\n",
    "<col width=\"12%\" />\n",
    "<col width=\"40%\" />\n",
    "<col width=\"24%\" />\n",
    "<col width=\"24%\" />\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Format Type</th>\n",
    "<th class=\"head\">Data Description</th>\n",
    "<th class=\"head\">Reader</th>\n",
    "<th class=\"head\">Writer</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">CSV</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-read-csv-table\"><span class=\"std std-ref\">read_csv</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-store-in-csv\"><span class=\"std std-ref\">to_csv</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"https://www.json.org/\">JSON</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-json-reader\"><span class=\"std std-ref\">read_json</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-json-writer\"><span class=\"std std-ref\">to_json</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>text</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-read-html\"><span class=\"std std-ref\">read_html</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-html\"><span class=\"std std-ref\">to_html</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>text</td>\n",
    "<td>Local clipboard</td>\n",
    "<td><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">read_clipboard</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">to_clipboard</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Microsoft_Excel\">MS Excel</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-excel-reader\"><span class=\"std std-ref\">read_excel</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-excel-writer\"><span class=\"std std-ref\">to_excel</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"http://www.opendocumentformat.org\">OpenDocument</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-ods\"><span class=\"std std-ref\">read_excel</span></a></td>\n",
    "<td>&#160;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://support.hdfgroup.org/HDF5/whatishdf5.html\">HDF5 Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">read_hdf</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">to_hdf</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://github.com/wesm/feather\">Feather Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">read_feather</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">to_feather</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://parquet.apache.org/\">Parquet Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">read_parquet</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">to_parquet</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://msgpack.org/index.html\">Msgpack</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-msgpack\"><span class=\"std std-ref\">read_msgpack</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-msgpack\"><span class=\"std std-ref\">to_msgpack</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stata\">Stata</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-stata-reader\"><span class=\"std std-ref\">read_stata</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-stata-writer\"><span class=\"std std-ref\">to_stata</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SAS_(software)\">SAS</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-sas-reader\"><span class=\"std std-ref\">read_sas</span></a></td>\n",
    "<td>&#160;</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>binary</td>\n",
    "<td><a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html\">Python Pickle Format</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">read_pickle</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">to_pickle</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>SQL</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">read_sql</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">to_sql</span></a></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>SQL</td>\n",
    "<td><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/BigQuery\">Google Big Query</a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-bigquery\"><span class=\"std std-ref\">read_gbq</span></a></td>\n",
    "<td><a class=\"reference internal\" href=\"#io-bigquery\"><span class=\"std std-ref\">to_gbq</span></a></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV\n",
    "Comma Separated Values files are text files that use commas as field delimeters.<br>\n",
    "Unless you're running the virtual environment included with the course, you may need to install <tt>xlrd</tt> and <tt>openpyxl</tt>.<br>\n",
    "In your terminal/command prompt run:\n",
    "\n",
    "    conda install xlrd  or pip install xlrd\n",
    "    conda install openpyxl  or pip install openpyxl\n",
    "\n",
    "Then restart Jupyter Notebook.\n",
    "(or use pip install if you aren't using the Anaconda Distribution)\n",
    "\n",
    "### Understanding File Paths\n",
    "\n",
    "You have two options when reading a file with pandas:\n",
    "\n",
    "1. If your .py file or .ipynb notebook is located in the **exact** same folder location as the .csv file you want to read, simply pass in the file name as a string, for example:\n",
    "    \n",
    "        df = pd.read_csv('some_file.csv')\n",
    "        \n",
    "2. Pass in the entire file path if you are located in a different directory. The file path must be 100% correct in order for this to work. For example:\n",
    "\n",
    "        df = pd.read_csv(\"C:\\\\Users\\\\myself\\\\files\\\\some_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd  # prit your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('example.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "Pandas can read table tabs off of HTML. This only works if your firewall isnb't blocking pandas from accessing the internet!\n",
    "\n",
    "Unless you're running the virtual environment included with the course, you may need to install <tt>lxml</tt>, <tt>htmllib5</tt>, and <tt>BeautifulSoup4</tt>.<br>\n",
    "In your terminal/command prompt run:\n",
    "\n",
    "    conda install lxml or pip install lxml\n",
    "    conda install html5lib or pip install html5lib\n",
    "    conda install beautifulsoup4 or pip install beautifulsoup4\n",
    "\n",
    "Then restart Jupyter Notebook.\n",
    "(or use pip install if you aren't using the Anaconda Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitf6340f14c0b044d39ba52dc528b569bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
